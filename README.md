ğŸ§  Gradio Chat Interface for Ollama Phi Model

This is a practice project that demonstrates how to build a simple chat interface using Gradio and the Ollama Phi model running locally.

It allows users to send messages and get AI-generated responses in a clean, interactive web interface.

ğŸš€ Features

ğŸ—£ï¸ Chat interactively with a local Phi model

ğŸ§¹ Clear the chat with one click

ğŸŒ Launches in a browser automatically

ğŸ§© Built with Gradio and OpenAIâ€™s Python client

ğŸ› ï¸ Requirements

Make sure you have the following installed:

Python 3.8+

Ollama
 (running locally)

Gradio

OpenAI Python package

You can install the required packages by running:

pip install gradio openai

â–¶ï¸ How to Run

Save the Python file (e.g., gradio_chat.py).

Make sure Ollama is running locally (ollama serve).

Run the script:

python gradio_chat.py


Wait for the Gradio interface to open in your browser.

ğŸ’¬ Example Use

You can start chatting immediately once the interface opens:

User: Hello!
AI: Hi there! How can I help you today?

ğŸ§° Notes

This is a practice project for learning how to connect Gradio with local models.

The API key "nokeyneeded" is just a placeholder for local testing.

Model used: "phi:latest" â€” can be changed to any model supported by your Ollama installation.