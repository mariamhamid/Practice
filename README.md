# 🧠 **Gradio Chat Interface for Ollama Phi Model**

This is a practice project that demonstrates how to build a simple chat interface using Gradio and the Ollama Phi model running locally. It allows users to send messages and get AI-generated responses in a clean, interactive web interface.

# **🚀 Features**

🗣️ Chat interactively with a local Phi model

🧹 Clear the chat with one click

🌐 Launches automatically in your browser

🧩 Built with Gradio and OpenAI’s Python client

# 🛠️ **Requirements**

Python 3.8+

Ollama
 (running locally)

Gradio

OpenAI Python package
Install dependencies:

pip install gradio openai

# ▶️**How to Run**

Save the Python file (e.g., gradio_chat.py)

Run Ollama locally:

ollama serve


# **Run the script:**

python gradio_chat.py


# **Access the Gradio chat interface in your browser.**

💬 Example Use
👩‍💻 User: Hello!  
🤖 AI: Hi there! How can I help you today?

# **🧰 Notes**

This is a practice project to learn how to connect Gradio with local models.

The API key "nokeyneeded" is just a placeholder.

# **Model used:** "phi:latest" — can be replaced with any Ollama-supported model.
