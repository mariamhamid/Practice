# ğŸ§  **Gradio Chat Interface for Ollama Phi Model**

This is a practice project that demonstrates how to build a simple chat interface using Gradio and the Ollama Phi model running locally. It allows users to send messages and get AI-generated responses in a clean, interactive web interface.

# **ğŸš€ Features**

ğŸ—£ï¸ Chat interactively with a local Phi model

ğŸ§¹ Clear the chat with one click

ğŸŒ Launches automatically in your browser

ğŸ§© Built with Gradio and OpenAIâ€™s Python client

# ğŸ› ï¸ **Requirements**

Python 3.8+

Ollama
 (running locally)

Gradio

OpenAI Python package
Install dependencies:

pip install gradio openai

# â–¶ï¸**How to Run**

Save the Python file (e.g., gradio_chat.py)

Run Ollama locally:

ollama serve


# **Run the script:**

python gradio_chat.py


# **Access the Gradio chat interface in your browser.**

ğŸ’¬ Example Use
ğŸ‘©â€ğŸ’» User: Hello!  
ğŸ¤– AI: Hi there! How can I help you today?

# **ğŸ§° Notes**

This is a practice project to learn how to connect Gradio with local models.

The API key "nokeyneeded" is just a placeholder.

# **Model used:** "phi:latest" â€” can be replaced with any Ollama-supported model.
